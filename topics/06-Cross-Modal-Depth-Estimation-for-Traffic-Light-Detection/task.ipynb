{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Task* - Cross-Modal Depth Estimation for Traffic Light Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Develop a methodology for estimating the 3D position of a traffic light, given an image bounding box and a lidar point cloud.\n",
    "\n",
    "- [Background and Motivation](#background-and-motivation)\n",
    "- [Task](#task)\n",
    "- [Required Tools and Data](#required-tools-and-data)\n",
    "- [Hints](#hints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Motivation\n",
    "\n",
    "An accurate knowledge about traffic light states is vital for fully-automated driving in urban environments. If traffic lights were equipped with the right hard- and software, V2X communication would be a reasonable way of transmitting traffic light states to automated vehicles nearby. As a fallback to V2X solutions or in situations, where V2X is not supported, it is also desirable to have a perception function dedicated to detecting traffic lights and their current state from camera images.\n",
    "\n",
    "The actual problem is two-fold: first, traffic lights need to be detected in camera images and their current state needs to be inferred; second, the algorithm needs to evaluate whether the detected traffic light has any influence on the currently planned trajectory or whether it can be ignored. This topic is supposed to partially deal with the second task, namely the estimation of the traffic light's position in 3D space. If the position was known, it could then be matched to a particular traffic light on an HD map in order evaluate whether the traffic light is important to the automated vehicle.\n",
    "\n",
    "Exact depth estimation from a single monocular camera image is not possible in closed-form due to ambiguities (e.g., there is no way to differentiate between a small object closer to the camera vs. a larger scaled version of the same object, which is further away). One way to obtain depth information is to use a stereoscopic camera pair. Another option is to make use of other sensors that, opposed to standard cameras, deliver three-dimensional information. In particular, this topic deals with the combination of camera and lidar point cloud data, in order to estimate the 3D position of a traffic light detected in the camera image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "The task is to develop a methodology for estimating the 3D position of a traffic light, given a binary image segmentation mask and a lidar point cloud.\n",
    "\n",
    "### Subtasks\n",
    "\n",
    "> ***Note:*** *The subtasks listed below do not have to be followed strictly. They serve the purpose of guiding you along your own research for this topic.*\n",
    "\n",
    "1. Make yourself familiar with the supplied (synthetic) dataset, containing:\n",
    "   - camera images\n",
    "   - binary image segmentation masks (traffic lights in white, everything else in black)\n",
    "   - lidar point clouds\n",
    "2. Research literature on existing approaches for mapping depth information from lidar point clouds to camera images or vice-versa.\n",
    "3. Develop an algorithm to estimate the 3D positions of all traffic lights included in the camera image by mapping the 3D point cloud information to the image.\n",
    "4. Apply your algorithm to the supplied data samples and qualitatively evaluate it.\n",
    "5. Implement a (C++ or Python) ROS node, which ...\n",
    "   1. subscribes to a segmentation map and corresponding point cloud\n",
    "   2. applies your developed algorithm to estimate the 3D positions of all traffic lights\n",
    "   3. publishes the estimated 3D position of each traffic light as a new ROS transform frame using a [TransformBroadcaster](http://wiki.ros.org/tf2/Tutorials/Writing%20a%20tf2%20broadcaster%20%28C%2B%2B%29)\n",
    "6. Apply your ROS node to the supplied data samples (ROS bag) and qualitatively evaluate it.\n",
    "7. Document your research, developed approach, and evaluations in a Jupyter notebook report. Explain and reproduce individual parts of your implemented functions with exemplary data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Tools and Data\n",
    "\n",
    "### Tools\n",
    "\n",
    "- ROS\n",
    "- *(potentially)* OpenCV\n",
    "\n",
    "### Data\n",
    "\n",
    "- [bag files including binary segmentation masks, point clouds, and camera images](data/bags/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "### Relevant ACDC Sections\n",
    "\n",
    "- **Sensor Data Processing Algorithms**\n",
    "  - Image Segmentation\n",
    "  - Camera-based Semantic Grid Mapping *(camera and transformation matrices)*\n",
    "\n",
    "### Transforms\n",
    "\n",
    "You will likely need to consider the geometric relation between camera and lidar, i.e., what is the transform between coordinates in camera coordinate system vs. coordinates in lidar coordinate system. We assume that this relation is known and thus include it with the supplied data. For the initial development of the algorithm, you may hard-code the supplied relation. In the ROS bag, we furthermore supply it via [ROS' tf-mechanism](http://wiki.ros.org/tf2/Tutorials), s.t. it can easily be fetched with code similar to the following:\n",
    "\n",
    "```cpp\n",
    "tf2_ros::Buffer tf_buffer;\n",
    "tf2_ros::TransformListener tf_listener(tf_buffer);\n",
    "geometry_msgs::TransformStamped transform = tf_buffer.lookupTransform(\"camera_link\", \"lidar_link\", ros::Time(0));\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f35c820e40155ce3a3d6b6baab4aa8cb626eff9596fe63e71a966e5e0dc1513e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('acdc-rp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
