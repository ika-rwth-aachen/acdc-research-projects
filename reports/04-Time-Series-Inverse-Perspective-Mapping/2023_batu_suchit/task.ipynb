{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Task* - Time-Series Inverse Perspective Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Develop a methodology to fuse camera image information from multiple consecutive time steps in order to compute an advanced semantic grid map using the geometry-based Inverse Perspective Mapping (IPM) approach.\n",
    "\n",
    "- [Background and Motivation](#background-and-motivation)\n",
    "- [Task](#task)\n",
    "- [Required Tools and Data](#required-tools-and-data)\n",
    "- [Hints](#hints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Motivation\n",
    "\n",
    "Grid maps play an important role in environment perception and may be used for purposes such as lane detection or free space detection. One way to compute semantic grid maps is to geometrically transform semantically segmented camera images using *Inverse Perspective Mapping (IPM)*. One exemplary semantic grid map computed from 8 semantically segmented camera images is shown below.\n",
    "\n",
    "![](./assets/ipm.png)\n",
    "\n",
    "The classical IPM approach has several shortcomings due to its assumption of a flat world:\n",
    "- objects with vertical extent (e.g., cars) are heavily distorted;\n",
    "- flat world assumption is often wrong even for seemingly flat surfaces like roads (leading to, e.g., non-parallel lane markers in grid map);\n",
    "- effective resolution drops with distance.\n",
    "\n",
    "One idea to improve on the basic IPM approach is to fuse camera image information from multiple consecutive time steps by involving information about the ego motion of the automated vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "The task is to develop a methodology to fuse camera image information from multiple consecutive time steps in order to compute an advanced semantic grid map using the geometry-based Inverse Perspective Mapping (IPM) approach.\n",
    "\n",
    "### Subtasks\n",
    "\n",
    "> ***Note:*** *The subtasks listed below do not have to be followed strictly. They serve the purpose of guiding you along your own research for this topic.*\n",
    "\n",
    "1. Implement a basic TensorFlow data pipeline.\n",
    "1. Implement a basic TensorFlow model for semantic image segmentation.\n",
    "1. Train basic TensorFlow models on the provided datasets for semantic image segmentation.\n",
    "   - either train separate models for front/left/right/rear camera images\n",
    "   - or train a single model for all cameras\n",
    "1. Using the trained models, compute semantic segmentation predictions for all input camera images.\n",
    "2. Apply IPM to ground-truth and predicted semantic segmentation images to obtain semantic grid map estimations.\n",
    "3. Research methods to combine semantic grid maps of multiple consecutive time steps using available ego motion information (e.g., current ego velocity).\n",
    "4. Develop an algorithm to fuse semantic grid maps of multiple consecutive time steps by also considering the ego motion of the automated vehicle.\n",
    "   - start by fusing the ground-truth semantic segmentation images\n",
    "   - add functionality to also fuse the predicted semantic segmentation images, such that semantic segmentation errors can also be corrected\n",
    "5. Evaluate the results of the advanced IPM algorithm in comparison to the single-shot IPM method and ground truth bird's eye view data (suggested metric: *Mean IoU*).\n",
    "   - evaluate performance on flat-world vs. static objects vs. dynamic objects\n",
    "   - evaluate dependence on the number of included time steps;\n",
    "   - evaluate dependence on the time delta between included time steps.\n",
    "   - ...\n",
    "6. Document your research, developed approach, and evaluations in a Jupyter notebook report. Explain and reproduce individual parts of your implemented functions with exemplary data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Tools and Data\n",
    "\n",
    "### Tools\n",
    "\n",
    "- TensorFlow\n",
    "- Image Segmentation Training Pipeline & Model *(see [ACDC Exercise: Semantic Image Segmentation](https://github.com/ika-rwth-aachen/acdc-notebooks/blob/main/section_2_sensor_data_processing/1_semantic_image_segmentation.ipynb))*\n",
    "- [Python IPM implementation from ika paper Cam2BEV](https://github.com/ika-rwth-aachen/Cam2BEV/tree/master/preprocessing/ipm)\n",
    "  - the `ipm.py` script allows you to compute a semantic grid map that is matching the viewport of the ground-truth drone camera\n",
    "- *(potentially)* OpenCV\n",
    "\n",
    "### Data\n",
    "\n",
    "- [two synthetic datasets](data/) containing consecutive samples of ...\n",
    "  - camera images (front, rear, left right)\n",
    "  - semantically segmented camera images (front, rear, left, right)\n",
    "    - ground truth for semantic image segmentation model(s)\n",
    "  - ground-truth semantically segmented drone camera images\n",
    "    - ground-truth for evaluation\n",
    "  - camera intrinsics/extrinsics\n",
    "  - ego motion of vehicle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "### Relevant ACDC Sections\n",
    "\n",
    "- **Sensor Data Processing Algorithms**\n",
    "  - Image Segmentation\n",
    "  - Camera-based Semantic Grid Mapping\n",
    "\n",
    "### Thoughts on Possible Fusion Algorithms\n",
    "\n",
    "> ***Note:*** *The suggestions detailed below do not have to be chosen for the developed methodology. They only serve as inspiration.*\n",
    "\n",
    "There is no obvious answer to the question of which information in which representation to fuse for this task. One reasonable option is to fuse the segmentation model's raw output, i.e., before it is converted to a semantic segmentation map using `argmax`. The raw model output usually is the output of a `softmax` activation, which can be interpreted to contain semantic class probabilities for every image pixel. The following code snippets walk you through the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install and import the required Python packages for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (1.22.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install \\\n",
    "    numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demo purposes, let's only consider two tiny 3x4 camera images, for which we predict pixel-level association to five semantic classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGES = 2\n",
    "IMAGE_SHAPE = (3, 4, 3)\n",
    "N_CLASSES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's only implement a dummy model function, yielding output as a semantic image segmentation model would. The `softmax`-outputs containing the class probabilities for each pixel can be converted to the final semantic segmentation map by applying `argmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dummy model function creating a random softmax output\n",
    "# (class dimension of each pixel sums to 1)\n",
    "def model(img):\n",
    "    random = np.random.random((img.shape[0], img.shape[1], N_CLASSES))\n",
    "    norm_over_class_dim = np.linalg.norm(random, ord=1, axis=-1)\n",
    "    softmax_output = random / np.expand_dims(norm_over_class_dim, axis=-1)\n",
    "    return softmax_output\n",
    "\n",
    "def modelOutputToSegmentationMap(model_output):\n",
    "    return np.argmax(model_output, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create two random images, compute the dummy model outputs and print some information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1\n",
      "  Image shape: (3, 4, 3)\n",
      "  Model output shape: (3, 4, 5)\n",
      "  Class probabilites for top-left pixel: [0.13317226 0.27574571 0.23495605 0.06773161 0.28839436]\n",
      "  Segmentation map (class indices): \n",
      "[[4 1 4 4]\n",
      " [4 0 2 2]\n",
      " [2 2 4 1]]\n",
      "Image 2\n",
      "  Image shape: (3, 4, 3)\n",
      "  Model output shape: (3, 4, 5)\n",
      "  Class probabilites for top-left pixel: [0.11428316 0.16525105 0.2131195  0.29853636 0.20880993]\n",
      "  Segmentation map (class indices): \n",
      "[[3 0 1 2]\n",
      " [3 2 2 1]\n",
      " [2 0 1 2]]\n"
     ]
    }
   ],
   "source": [
    "# print information about camera images and computed segmentation maps\n",
    "model_outputs = []\n",
    "for i in range(N_IMAGES):\n",
    "    camera_image = np.random.random(IMAGE_SHAPE)\n",
    "    model_output = model(camera_image)\n",
    "    model_outputs.append(model_output)\n",
    "    segmentation_map = modelOutputToSegmentationMap(model_output)\n",
    "    print(f\"Image {i+1}\")\n",
    "    print(f\"  Image shape: {camera_image.shape}\")\n",
    "    print(f\"  Model output shape: {model_output.shape}\")\n",
    "    print(f\"  Class probabilites for top-left pixel: {model_output[0, 0, :]}\")\n",
    "    print(f\"  Segmentation map (class indices): \\n{segmentation_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of fusing the final segmentation maps, where each pixel has already been assigned one particular class, we can fuse the information of the two images one step earlier by averaging the class probabilities in the model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused model output\n",
      "  Averaged probabilites for top-left pixel: [0.12372771 0.22049838 0.22403778 0.18313399 0.24860215]\n",
      "  Averaged segmentation map (class indices): \n",
      "[[4 1 4 2]\n",
      " [3 0 2 1]\n",
      " [2 1 4 2]]\n"
     ]
    }
   ],
   "source": [
    "def fuseModelOutput(model_outputs):\n",
    "    return sum(model_outputs) / len(model_outputs)\n",
    "\n",
    "fused_model_output = fuseModelOutput(model_outputs)\n",
    "fused_segmentation_map = modelOutputToSegmentationMap(fused_model_output)\n",
    "print(f\"Fused model output\")\n",
    "print(f\"  Averaged probabilites for top-left pixel: {fused_model_output[0, 0, :]}\")\n",
    "print(f\"  Averaged segmentation map (class indices): \\n{fused_segmentation_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ego motion of the automated vehicle was not considered in this example. If applied to the problem at hand, one would want to first shift the second semantic segmentation class probability tensor in the direction of travel (as extracted from ego motion).\n",
    "\n",
    "Additionally, instead of averaging class probabilities, one could also incorporate heuristic rules such as: *if at least one of my considered samples is predicting 'road' for a particular region, then always assume 'road' for that region in the final output*. This could potentially lead to a mapping of the static world, where dynamic objects would be filtered out. The elimination of dynamic objects is one possible successful outcome of this research project."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f35c820e40155ce3a3d6b6baab4aa8cb626eff9596fe63e71a966e5e0dc1513e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
